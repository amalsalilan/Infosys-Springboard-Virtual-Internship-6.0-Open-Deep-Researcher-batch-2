{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalsalilan/Infosys-Springboard-Virtual-Internship-6.0-Open-Deep-Researcher-batch-2/blob/Nischay_Pandey/Milestone1_ScopePhase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS6VmwpQxjTD"
      },
      "source": [
        "Cell 1 â€” Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSJfG3aTxWoa"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph google-generativeai tavily-python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_E7nvvx41H"
      },
      "source": [
        "cell 2:- API Keys Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eshOywZzx6Hf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tavily import TavilyClient\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ðŸ”‘ Replace (abc & xyz) with real keys\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"abc\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"xyz\"\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21LxyXwyHAS"
      },
      "source": [
        "Cell 3 â€” Research State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJEsNQXlyLpL"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class ResearchState(BaseModel):\n",
        "    query: str\n",
        "    clarification_needed: bool = False\n",
        "    follow_up_questions: Optional[List[str]] = None\n",
        "    clarified_query: Optional[str] = None\n",
        "    search_results: Optional[List[dict]] = None\n",
        "    summary: Optional[str] = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcCWDDS7yqrj"
      },
      "source": [
        "Cell 4 â€” Utility: safe response extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_CoTHT9yr8p"
      },
      "outputs": [],
      "source": [
        "def estimate_tokens(text: str) -> int:\n",
        "    \"\"\"Rough token estimator.\"\"\"\n",
        "    return int(len(text.split()) * 1.3)\n",
        "\n",
        "def safe_print_json(data):\n",
        "    \"\"\"Nicely print dicts/lists like an AI agent response.\"\"\"\n",
        "    import json\n",
        "    print(json.dumps(data, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40a5Jw3y9HQ"
      },
      "source": [
        "Cell 5 â€” Clarification agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyhznmOiy7Qh"
      },
      "outputs": [],
      "source": [
        "def clarification_agent(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Decide if the query is clear enough or follow-up questions are needed.\n",
        "    Uses Gemini to judge only when necessary.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful research assistant.\n",
        "Given this user query:\n",
        "\n",
        "\"{state.query}\"\n",
        "\n",
        "Decide if clarification is needed.\n",
        "If yes, ask **max 2 clear and specific follow-up questions**.\n",
        "If not, confirm it's sufficient.\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "  \"clarification_needed\": true/false,\n",
        "  \"follow_up_questions\": [..] or null\n",
        "}}\n",
        "\"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    try:\n",
        "        parsed = response.text.strip()\n",
        "        import json\n",
        "        parsed_json = json.loads(parsed)\n",
        "        state.clarification_needed = parsed_json.get(\"clarification_needed\", False)\n",
        "        state.follow_up_questions = parsed_json.get(\"follow_up_questions\")\n",
        "    except Exception:\n",
        "        # fallback: assume no clarification\n",
        "        state.clarification_needed = False\n",
        "        state.follow_up_questions = None\n",
        "\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j05Ce4PpzLez"
      },
      "source": [
        "Cell 6 â€” Query generator node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTFV0crgzJVp"
      },
      "outputs": [],
      "source": [
        "def query_generator(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Generate a clarified search query if clarification was answered.\n",
        "    \"\"\"\n",
        "    if not state.clarified_query:\n",
        "        state.clarified_query = state.query\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70S5kithze4P"
      },
      "source": [
        "Cell 7 â€” research pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3pedJ6_zrUv"
      },
      "outputs": [],
      "source": [
        "def research_pipeline(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Runs Tavily search if query is broad/needs context,\n",
        "    otherwise simulates LLM reasoning without web search.\n",
        "    \"\"\"\n",
        "    # Simple rule: only call Tavily if query looks factual/research heavy\n",
        "    keywords = [\"latest\", \"statistics\", \"research\", \"compare\", \"trends\", \"report\"]\n",
        "    if any(kw in state.clarified_query.lower() for kw in keywords):\n",
        "        print(\"ðŸ”Ž Running Tavily search...\")\n",
        "        search = tavily_client.search(state.clarified_query, max_results=3)\n",
        "        state.search_results = search.get(\"results\", [])\n",
        "    else:\n",
        "        print(\"âš¡ Skipping web search (not needed).\")\n",
        "        state.search_results = []\n",
        "\n",
        "    # Summarize with Gemini\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    context = \"\"\n",
        "    if state.search_results:\n",
        "        for r in state.search_results:\n",
        "            context += f\"- {r.get('title','')} :: {r.get('content','')[:200]}\\n\"\n",
        "\n",
        "    summary_prompt = f\"\"\"\n",
        "User query: {state.clarified_query}\n",
        "\n",
        "Context (may be empty):\n",
        "{context}\n",
        "\n",
        "Provide a clear, agent-like research summary.\n",
        "\"\"\"\n",
        "    response = model.generate_content(summary_prompt)\n",
        "    state.summary = response.text.strip()\n",
        "\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p--gWOi-0J5L"
      },
      "source": [
        "Cell 8 â€” LangGraph Flow and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf_eFcEq0L3X"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(ResearchState)\n",
        "\n",
        "workflow.add_node(\"clarification\", clarification_agent)\n",
        "workflow.add_node(\"query_gen\", query_generator)\n",
        "workflow.add_node(\"pipeline\", research_pipeline)\n",
        "\n",
        "workflow.set_entry_point(\"clarification\")\n",
        "workflow.add_edge(\"clarification\", \"query_gen\")\n",
        "workflow.add_edge(\"query_gen\", \"pipeline\")\n",
        "workflow.add_edge(\"pipeline\", END)\n",
        "\n",
        "graph = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRB3NmoV0ZWh"
      },
      "source": [
        "Cell 9 â€” Memory initialization and chat() function (single-turn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRdsfsu90XGu"
      },
      "outputs": [],
      "source": [
        "def decide_search(state: ResearchState) -> bool:\n",
        "    return bool(state.search_results)\n",
        "\n",
        "def extract_facts_with_gemini(text: str):\n",
        "    return []  # milestone 2+\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSZKmyI50xl0"
      },
      "source": [
        "Cell 10 â€” continuous chatbot loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixntL6zZ01uz"
      },
      "outputs": [],
      "source": [
        "def chatbot():\n",
        "    print(\"ðŸ¤– Research Agent Ready (Milestone 1)\\n\")\n",
        "    while True:\n",
        "        query = input(\"You: \").strip()\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"ðŸ‘‹ Ending session.\")\n",
        "            break\n",
        "\n",
        "        state = ResearchState(query=query)\n",
        "\n",
        "        # Step 1: Clarification\n",
        "        state = clarification_agent(state)\n",
        "        if state.clarification_needed and state.follow_up_questions:\n",
        "            print(\"\\nðŸ¤– I need a bit more info:\")\n",
        "            for q in state.follow_up_questions:\n",
        "                print(\" -\", q)\n",
        "            ans = input(\"\\nYour clarification: \").strip()\n",
        "            state.clarified_query = f\"{state.query} | Clarified: {ans}\"\n",
        "        else:\n",
        "            state.clarified_query = state.query\n",
        "\n",
        "        # Step 2: Query & Pipeline\n",
        "        state = query_generator(state)\n",
        "        state = research_pipeline(state)\n",
        "\n",
        "        # Step 3: Final Output\n",
        "        print(\"\\nðŸ“Œ Research Summary:\")\n",
        "        print(state.summary)\n",
        "        print(\"\\n---\\n\")\n",
        "\n",
        "chatbot()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPByESXoP55PgwbaFlCY8QN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}