{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH8uW9zaQIVmrYEp9UEsfV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalsalilan/Infosys-Springboard-Virtual-Internship-6.0-Open-Deep-Researcher-batch-2/blob/Tejas_V/OpenChatResponse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HGLJ3dS6nBi",
        "outputId": "517988f3-df9b-4540-b92b-83c3a171ad02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langgraph google-generativeai tavily-python --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import json, re, traceback, time\n",
        "from typing import TypedDict\n",
        "\n",
        "# LangGraph StateGraph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Google Gemini client\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Tavily client\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Enter API keys\n",
        "GENAI_API_KEY = getpass(\"Enter Google Gemini API Key: \")\n",
        "TAVILY_API_KEY = getpass(\"Enter Tavily API Key: \")\n",
        "\n",
        "# Configure clients\n",
        "genai.configure(api_key=GENAI_API_KEY)\n",
        "tavily = TavilyClient(api_key=TAVILY_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiDSPtnX7Nhg",
        "outputId": "cf951c25-7120-4ecf-a3ed-7b6be0ac749d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Google Gemini API Key: ··········\n",
            "Enter Tavily API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResearchState(TypedDict, total=False):\n",
        "    user_input: str\n",
        "    clarification: str\n",
        "    query: str\n",
        "    summary: str\n",
        "    pipeline: str"
      ],
      "metadata": {
        "id": "jy0wYjzY7Y_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def safe_extract_genai_text(response):\n",
        "    \"\"\"\n",
        "    Given a genai response object, try several known access patterns and return plain text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        if hasattr(response, \"text\") and response.text:\n",
        "            return response.text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "\n",
        "        cand = response.candidates\n",
        "\n",
        "        if hasattr(cand, \"content\"):\n",
        "\n",
        "            content = getattr(cand, \"content\", None)\n",
        "            if content and hasattr(content, \"parts\"):\n",
        "                parts = content.parts\n",
        "                if parts and len(parts) > 0 and getattr(parts, \"text\", None):\n",
        "                    return parts.text.strip()\n",
        "\n",
        "        if hasattr(cand, \"text\") and cand.text:\n",
        "            return cand.text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "        return str(response)\n",
        "    except:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "YMNbEf9v7k0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clarification_agent(state: ResearchState) -> ResearchState:\n",
        "    user_input = state.get(\"user_input\", \"\").strip()\n",
        "    if not user_input:\n",
        "        state[\"clarification\"] = \"Could you type your question?\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    if re.search(r\"\\bmy name is\\b\", user_input.lower()):\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "        return state\n",
        "\n",
        "    if re.search(r\"\\b(previous|last)\\s*(que|question|query|sawal)\\b\", user_input.lower()):\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a system that assesses clarity of user research questions.\n",
        "\n",
        "Question: \"{user_input}\"\n",
        "\n",
        "Classify the question into one of:\n",
        "- clear\n",
        "- vague_guessable\n",
        "- too_vague\n",
        "\n",
        "If you return \"vague_guessable\", provide a short \"refined_question\" that is a reasonable interpretation.\n",
        "Respond ONLY in JSON with keys: \"status\" and \"refined_question\" (string or empty).\n",
        "Example:\n",
        "{{\"status\":\"vague_guessable\", \"refined_question\":\"...\"}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "\n",
        "        parsed = {}\n",
        "        try:\n",
        "            parsed = json.loads(text_out)\n",
        "        except Exception:\n",
        "\n",
        "            m = re.search(r\"\\{.*\\}\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except:\n",
        "                    parsed = {}\n",
        "        status = parsed.get(\"status\", \"\").lower()\n",
        "        refined = parsed.get(\"refined_question\", \"\").strip()\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "        status, refined = \"clear\", \"\"\n",
        "\n",
        "    if status == \"clear\":\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "    elif status == \"vague_guessable\" and refined:\n",
        "        state[\"clarification\"] = refined\n",
        "    else:\n",
        "        state[\"clarification\"] = \"Could you provide more details about your question?\"\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "Do617uCw7n0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def query_generator(state: ResearchState) -> ResearchState:\n",
        "    clarification = state.get(\"clarification\", \"\")\n",
        "    user_input = state.get(\"user_input\", \"\")\n",
        "\n",
        "    if clarification == \"This request is clear\":\n",
        "        state[\"query\"] = user_input\n",
        "    elif clarification.startswith(\"Could you provide\"):\n",
        "        state[\"query\"] = f\"{user_input} (needs clarification: {clarification})\"\n",
        "    else:\n",
        "\n",
        "        state[\"query\"] = clarification or user_input\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "ATs9vteg9QBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decide_search(query: str) -> bool:\n",
        "\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "You are a decision module. Given a research question, answer whether it requires real-time web search\n",
        "or can be answered from general knowledge (no web search). Return JSON: {{\"need_search\": true/false}}.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\"\"\"\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "\n",
        "        parsed = {}\n",
        "        try:\n",
        "            parsed = json.loads(text_out)\n",
        "        except:\n",
        "            m = re.search(r\"\\{.*\\}\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except:\n",
        "                    parsed = {}\n",
        "        return bool(parsed.get(\"need_search\", True))\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "def tavily_search(query: str, max_results: int = 5):\n",
        "    \"\"\"\n",
        "    Wrapper for TavilyClient search.\n",
        "    Returns a list of result dicts or a string description if the client isn't available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        if hasattr(tavily, \"search\"):\n",
        "            results = tavily.search(query, max_results=max_results)\n",
        "            return results\n",
        "        elif hasattr(tavily, \"query\"):\n",
        "            results = tavily.query(query, max_results=max_results)\n",
        "            return results\n",
        "        else:\n",
        "\n",
        "            if hasattr(tavily, \"run\"):\n",
        "                return tavily.run(query)\n",
        "\n",
        "            return f\"\"\n",
        "    except Exception as e:\n",
        "        return f\"\"\n",
        "\n",
        "def research_pipeline(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Main research node - decides memory shortcuts, whether to use web search,\n",
        "    and produces a short summary (or placeholder).\n",
        "    \"\"\"\n",
        "    global memory\n",
        "    query = (state.get(\"query\") or \"\").strip()\n",
        "    if not query:\n",
        "        state[\"pipeline\"] = \"No query provided.\"\n",
        "        state[\"summary\"] = \"No summary available.\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    if re.search(r\"\\b(previous|last)\\s*(que|question|query|sawal)\\b\", query.lower()):\n",
        "        if memory[\"history\"]:\n",
        "            last_q = memory[\"history\"][-1][\"Q\"]\n",
        "            state[\"pipeline\"] = \"Retrieved from memory (history).\"\n",
        "            state[\"summary\"] = f\"Your previous question was: '{last_q}'\"\n",
        "        else:\n",
        "            state[\"pipeline\"] = \"Memory empty.\"\n",
        "            state[\"summary\"] = \"There is no previous question in memory.\"\n",
        "        return state\n",
        "\n",
        "    if \"my name\" in query.lower():\n",
        "        name = memory[\"facts\"].get(\"name\", \"I don’t know yet.\")\n",
        "        state[\"pipeline\"] = \"Retrieved from memory (facts).\"\n",
        "        state[\"summary\"] = f\"Your name is {name}.\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    facts = memory.get(\"facts\", {})\n",
        "    if query.lower() in (k.lower() for k in facts.keys()):\n",
        "        matched = next((v for k, v in facts.items() if k.lower() == query.lower()), None)\n",
        "        state[\"pipeline\"] = f\"Retrieved from memory: {matched}\"\n",
        "        state[\"summary\"] = matched\n",
        "        return state\n",
        "\n",
        "\n",
        "    need_search = decide_search(query)\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    try:\n",
        "        if need_search:\n",
        "\n",
        "            tavily_results = tavily_search(query, max_results=5)\n",
        "\n",
        "            combined_info = f\"Query: {query}\\n\\nWeb results (short): {json.dumps(tavily_results, default=str)[:4000]}\\n\\nSummarize the key findings in 3-5 bullet points.\"\n",
        "            response = model.generate_content(combined_info)\n",
        "            text_out = safe_extract_genai_text(response)\n",
        "            state[\"pipeline\"] = \"Tavily + Gemini\"\n",
        "            state[\"summary\"] = text_out\n",
        "        else:\n",
        "\n",
        "            prompt = f\"Query: {query}\\nProvide a concise answer or short summary (3-5 lines).\"\n",
        "            response = model.generate_content(prompt)\n",
        "            text_out = safe_extract_genai_text(response)\n",
        "            state[\"pipeline\"] = \"Gemini Only\"\n",
        "            state[\"summary\"] = text_out\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        state[\"pipeline\"] = f\"Error during research: {str(e)}\"\n",
        "        state[\"summary\"] = \"An error occurred while fetching results.\"\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "-OQkYBAf9fHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "graph = StateGraph(ResearchState)\n",
        "\n",
        "\n",
        "graph.add_node(\"ClarificationAgent\", clarification_agent)\n",
        "graph.add_node(\"QueryGenerator\", query_generator)\n",
        "graph.add_node(\"ResearchPipeline\", research_pipeline)\n",
        "\n",
        "\n",
        "graph.set_entry_point(\"ClarificationAgent\")\n",
        "graph.add_edge(\"ClarificationAgent\", \"QueryGenerator\")\n",
        "graph.add_edge(\"QueryGenerator\", \"ResearchPipeline\")\n",
        "\n",
        "\n",
        "app = graph.compile()\n",
        "print(\"✅ StateGraph compiled successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7YivZC89hHW",
        "outputId": "8272e303-c883-401d-9703-5bfea50328f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ StateGraph compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize memory and provide a chat() function to process one input at a time\n",
        "memory = {\n",
        "    \"facts\": {},    # Persistent knowledge (like user name, facts)\n",
        "    \"history\": []   # Conversation log\n",
        "}\n",
        "\n",
        "def extract_facts_with_gemini(text: str):\n",
        "    \"\"\"\n",
        "    Use Gemini to extract personal facts in JSON-list format: [{\"key\":\"...\", \"value\":\"...\"}]\n",
        "    Fallbacks are safe and non-fatal.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    prompt = f\"\"\"\n",
        "Extract any personal facts (name, age, location, role, company) from the following user sentence.\n",
        "Return a JSON list of objects with \"key\" and \"value\". If none, return.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "\n",
        "        facts = []\n",
        "        try:\n",
        "            facts = json.loads(text_out)\n",
        "        except:\n",
        "\n",
        "            m = re.search(r\"\\[.*\\]\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    facts = json.loads(m.group(0))\n",
        "                except:\n",
        "                    facts = []\n",
        "        if not isinstance(facts, list):\n",
        "            facts = []\n",
        "        return facts\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def chat(user_input: str, remember_name_rule: bool = True):\n",
        "\n",
        "    global memory\n",
        "\n",
        "\n",
        "    if remember_name_rule and re.search(r\"\\bmy name is\\b\", user_input.lower()):\n",
        "\n",
        "        try:\n",
        "            name_parts = user_input.lower().split(\"my name is\", 1)[1].strip().split()\n",
        "            if name_parts:\n",
        "              memory[\"facts\"][\"name\"] = name_parts[0].capitalize()\n",
        "              print(f\"✅ Stored name='{memory['facts']['name']}' in memory.\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "\n",
        "        try:\n",
        "            facts_list = extract_facts_with_gemini(user_input)\n",
        "            for f in facts_list:\n",
        "                key = f.get(\"key\", \"\").lower().strip()\n",
        "                value = f.get(\"value\", \"\").strip()\n",
        "                if key and value:\n",
        "                    memory[\"facts\"][key] = value\n",
        "                    print(f\"✅ I'll remember your {key} = {value}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "    if user_input.lower().startswith(\"what is my\"):\n",
        "        key = user_input.lower().replace(\"what is my\", \"\").strip()\n",
        "        val = memory[\"facts\"].get(key, \"I don’t know yet.\")\n",
        "        print(f\"Memory: {val}\")\n",
        "        return {\"user_input\": user_input, \"clarification\": \"\", \"query\": \"\", \"pipeline\": \"recall\", \"summary\": val}\n",
        "\n",
        "\n",
        "    state: ResearchState = {\n",
        "        \"user_input\": user_input,\n",
        "        \"clarification\": \"\",\n",
        "        \"query\": \"\",\n",
        "        \"summary\": \"\",\n",
        "        \"pipeline\": \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        state = app.invoke(state)\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        state[\"pipeline\"] = f\"Graph invocation error: {str(e)}\"\n",
        "        state[\"summary\"] = \"\"\n",
        "\n",
        "\n",
        "    print(\"\\n###  Clarification Agent\")\n",
        "    print(state.get(\"clarification\", \"\"))\n",
        "    print(\"\\n###  Final Research Query\")\n",
        "    print(state.get(\"query\", \"\"))\n",
        "    print(\"\\n###  Research Pipeline\")\n",
        "    print(state.get(\"pipeline\", \"\"))\n",
        "    print(\"\\n###  Final Summary\")\n",
        "    print(state.get(\"summary\", \"\"))\n",
        "\n",
        "\n",
        "    memory[\"history\"].append({\n",
        "        \"timestamp\": time.time(),\n",
        "        \"Q\": user_input,\n",
        "        \"clarification\": state.get(\"clarification\", \"\"),\n",
        "        \"query\": state.get(\"query\", \"\"),\n",
        "        \"pipeline\": state.get(\"pipeline\", \"\"),\n",
        "        \"A\": state.get(\"summary\", \"\")\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "5ZksbRPC94JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\" OpenDeepResearcher Chatbot (type 'quit' or 'exit' to stop)\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\" Goodbye! Session ended.\")\n",
        "            break\n",
        "\n",
        "\n",
        "        state = chat(user_input)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n Interrupted. Goodbye!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "sywa59zY98JS",
        "outputId": "fa2efa4b-4597-4c84-ae1c-0abc08d60bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " OpenDeepResearcher Chatbot (type 'quit' or 'exit' to stop)\n",
            "\n",
            "You: who is the best footballer of the world current\n",
            "\n",
            "###  Clarification Agent\n",
            "Who is currently considered the best footballer in the world, based on widely accepted awards and rankings?\n",
            "\n",
            "###  Final Research Query\n",
            "Who is currently considered the best footballer in the world, based on widely accepted awards and rankings?\n",
            "\n",
            "###  Research Pipeline\n",
            "Tavily + Gemini\n",
            "\n",
            "###  Final Summary\n",
            "* **Rodri is the current Ballon d'Or winner (2024):**  Multiple sources cite Rodri as the recipient of the 2024 Ballon d'Or award, making him the current holder of the prestigious title.\n",
            "\n",
            "* **Ousmane Dembélé is a leading contender for the 2025 Ballon d'Or:** Several sources place Dembélé at the top or near the top of their 2025 Ballon d'Or power rankings.\n",
            "\n",
            "* **Lamine Yamal is also a prominent contender for 2025:**  He consistently appears high in various 2025 Ballon d'Or prediction lists.\n",
            "\n",
            "* **No single definitive \"best\" player exists beyond the current Ballon d'Or winner:**  The various rankings and predictions for the 2025 Ballon d'Or highlight the subjective nature of the \"best\" footballer title, with several players vying for top position.\n",
            "\n",
            "* **Rankings are forward-looking, predicting 2025:**  Most results focus on predicting the 2025 Ballon d'Or, not definitively stating the current \"best\" based on current performance.  The current best is generally considered to be Rodri, based on the 2024 award.\n",
            "\n",
            " Interrupted. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFkvdsRyCOXH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}